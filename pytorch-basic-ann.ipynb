{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samyak15jain/pytorch-basic-ann?scriptVersionId=104780518\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Basic PyTorch Neural Network\n","metadata":{}},{"cell_type":"markdown","source":"**Note: I have done all the visualizations in another notebook( the svm one). Was not able to do the visualizations here because the Ram was occupied by the Neural Network(around 12.6 GB)**","metadata":{}},{"cell_type":"markdown","source":"**Used Pytorch for this project**","metadata":{}},{"cell_type":"markdown","source":"# Version 8","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.016117Z","iopub.execute_input":"2022-09-02T14:08:22.016689Z","iopub.status.idle":"2022-09-02T14:08:22.877394Z","shell.execute_reply.started":"2022-09-02T14:08:22.016638Z","shell.execute_reply":"2022-09-02T14:08:22.876603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()\n# Using a GPU for faster calculations","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.879336Z","iopub.execute_input":"2022-09-02T14:08:22.879796Z","iopub.status.idle":"2022-09-02T14:08:22.968845Z","shell.execute_reply.started":"2022-09-02T14:08:22.879742Z","shell.execute_reply":"2022-09-02T14:08:22.968061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_allocated()\n# After the notebook was run numerous times, to check the memory alloted to GPU as kaggle only allows 16GB.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.970245Z","iopub.execute_input":"2022-09-02T14:08:22.970509Z","iopub.status.idle":"2022-09-02T14:08:22.990475Z","shell.execute_reply.started":"2022-09-02T14:08:22.970469Z","shell.execute_reply":"2022-09-02T14:08:22.989562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a model class\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features=9, h1=250, h2=250,h3=50,h4=250,h5=250, out_features=2):\n# Used a really large number of neurons for this task.\n        super().__init__()\n        self.fc1 = nn.Linear(in_features,h1)    # input layer\n        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n        self.fc3 = nn.Linear(h2, h3)            # hidden layer\n        self.fc4 = nn.Linear(h3, h4)            # hidden layer\n        self.fc5 = nn.Linear(h4, h5)            # hidden layer\n        self.out = nn.Linear(h5, out_features)  # output layer\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = self.out(x)\n        return x\n#     Used ReLu activation function. Also used Sigmoid Function earlier but was not able to get the desired results.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.993262Z","iopub.execute_input":"2022-09-02T14:08:22.993831Z","iopub.status.idle":"2022-09-02T14:08:23.002327Z","shell.execute_reply.started":"2022-09-02T14:08:22.993683Z","shell.execute_reply":"2022-09-02T14:08:23.001562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiated the Model class using parameter defaults:\ntorch.manual_seed(32)\nmodel = Model()\ngpumodel= model.cuda()\n# Instantiated the model on GPU","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:23.004067Z","iopub.execute_input":"2022-09-02T14:08:23.004291Z","iopub.status.idle":"2022-09-02T14:08:29.84254Z","shell.execute_reply.started":"2022-09-02T14:08:23.004269Z","shell.execute_reply":"2022-09-02T14:08:29.841654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nasa-nearest-earth-objects/neo.csv')\ndf = df.drop('name',axis=1)\ndf = df.drop('sentry_object',axis=1)\ndf = df.drop('orbiting_body',axis=1)\ndf = df.drop('id',axis=1)\n# Dropping not so important classes. As sentry object is false and orbiting object is Earth.\n\n#  Feature Engineering to add more accuracy.\ndf[\"hazardous\"] = df[\"hazardous\"].astype(int)\ndf['Mass'] = df.loc[:, 'absolute_magnitude'] ** (1/4)\ndf['avg_radius'] = (df.loc[:, 'est_diameter_min'] + df.loc[:,'est_diameter_max'])/2\ndf['Volume'] = (df.loc[:, 'avg_radius'] ** 3)*1.33\ndf['energy'] = (df.loc[:, 'Mass'] * df.loc[:, 'relative_velocity']**2)*0.5\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:29.844158Z","iopub.execute_input":"2022-09-02T14:08:29.844528Z","iopub.status.idle":"2022-09-02T14:08:30.176542Z","shell.execute_reply.started":"2022-09-02T14:08:29.844497Z","shell.execute_reply":"2022-09-02T14:08:30.175747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Perform Train/Test/Split","metadata":{}},{"cell_type":"code","source":"# Using the scikit learn library.\nX = df.drop('hazardous',axis=1).values\n# from sklearn.preprocessing import StandardScaler\n# sc = StandardScaler()\n# X = sc.fit_transform(X)\ny = df['hazardous'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=33)\n\nX_train = torch.FloatTensor(X_train).cuda()\nX_test = torch.FloatTensor(X_test).cuda()\n# y_train = F.one_hot(torch.LongTensor(y_train))\n# y_test = F.one_hot(torch.LongTensor(y_test))\n# Not needed with cross entropy loss, was earlier implementing BCELoss but was facing error while converting tensors from long to float.\ny_train = torch.LongTensor(y_train).cuda()\ny_test = torch.LongTensor(y_test).cuda()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.177943Z","iopub.execute_input":"2022-09-02T14:08:30.178227Z","iopub.status.idle":"2022-09-02T14:08:30.210533Z","shell.execute_reply.started":"2022-09-02T14:08:30.178179Z","shell.execute_reply":"2022-09-02T14:08:30.209845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.213557Z","iopub.execute_input":"2022-09-02T14:08:30.21376Z","iopub.status.idle":"2022-09-02T14:08:30.23727Z","shell.execute_reply.started":"2022-09-02T14:08:30.213737Z","shell.execute_reply":"2022-09-02T14:08:30.23661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ndata = df.drop('hazardous',axis=1).values\nlabels = df['hazardous'].values\nclass_weights =[]\n\nDataset = TensorDataset(torch.FloatTensor(data).cuda(),torch.LongTensor(labels).cuda())","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.23837Z","iopub.execute_input":"2022-09-02T14:08:30.239265Z","iopub.status.idle":"2022-09-02T14:08:30.250547Z","shell.execute_reply.started":"2022-09-02T14:08:30.239227Z","shell.execute_reply":"2022-09-02T14:08:30.249856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data_loader = DataLoader(Dataset, batch_size=105, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.25404Z","iopub.execute_input":"2022-09-02T14:08:30.254281Z","iopub.status.idle":"2022-09-02T14:08:30.259755Z","shell.execute_reply.started":"2022-09-02T14:08:30.254249Z","shell.execute_reply":"2022-09-02T14:08:30.258833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torchsampler","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:26:40.420974Z","iopub.execute_input":"2022-09-01T14:26:40.421489Z","iopub.status.idle":"2022-09-01T14:26:40.429634Z","shell.execute_reply.started":"2022-09-01T14:26:40.421457Z","shell.execute_reply":"2022-09-01T14:26:40.428707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchsampler import ImbalancedDatasetSampler\n\n# train_loader = torch.utils.data.DataLoader(\n#     iris,\n#     sampler=ImbalancedDatasetSampler(iris),\n#     batch_size=1000,\n#     **kwargs\n# )\n\n# Was trying to use Imbalance Data Sampler from github but faced some error, so manually entered class weights.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:26:40.431168Z","iopub.execute_input":"2022-09-01T14:26:40.432124Z","iopub.status.idle":"2022-09-01T14:26:40.439332Z","shell.execute_reply.started":"2022-09-01T14:26:40.432086Z","shell.execute_reply":"2022-09-01T14:26:40.438579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define loss equations and optimizations\n","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(4)\nmodel = Model()\ngpumodel= model.cuda()\nweight = torch.FloatTensor([0.9,0.09]).cuda()\n# Assigned weights to classes as the data provided was highly imbalanced.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:15.293132Z","iopub.execute_input":"2022-09-01T14:28:15.293437Z","iopub.status.idle":"2022-09-01T14:28:15.303707Z","shell.execute_reply.started":"2022-09-01T14:28:15.293394Z","shell.execute_reply":"2022-09-01T14:28:15.302864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:16.346442Z","iopub.execute_input":"2022-09-01T14:28:16.347129Z","iopub.status.idle":"2022-09-01T14:28:16.357199Z","shell.execute_reply.started":"2022-09-01T14:28:16.347096Z","shell.execute_reply":"2022-09-01T14:28:16.356345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight = weight).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000000001)\n# Since the dataset was highly imbalanced was forced to use a extremely low learning rate or else the model was predicting all the values as either 1 or 0.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:18.513425Z","iopub.execute_input":"2022-09-01T14:28:18.51399Z","iopub.status.idle":"2022-09-01T14:28:18.519879Z","shell.execute_reply.started":"2022-09-01T14:28:18.513957Z","shell.execute_reply":"2022-09-01T14:28:18.518513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"epochs = 3000000\nlosses = []\n# Used too many epochs to further lower down the losses.\n# The training went for around 9 hours using Kaggle Tesla GPUs.\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model.forward(X_train)\n    loss = criterion(y_pred, y_train)\n    losses.append(loss)\n    \n    # a neat trick to save screen space:\n    if i%1000 == 1:\n        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:19.322983Z","iopub.execute_input":"2022-09-01T14:28:19.323261Z","iopub.status.idle":"2022-09-01T19:12:49.348394Z","shell.execute_reply.started":"2022-09-01T14:28:19.323231Z","shell.execute_reply":"2022-09-01T19:12:49.346485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the loss function","metadata":{}},{"cell_type":"code","source":"epochs = 720465","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:16:03.890422Z","iopub.execute_input":"2022-09-01T19:16:03.891145Z","iopub.status.idle":"2022-09-01T19:16:03.895959Z","shell.execute_reply.started":"2022-09-01T19:16:03.891112Z","shell.execute_reply":"2022-09-01T19:16:03.894044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(epochs), losses)\nplt.ylabel('Loss')\nplt.xlabel('epoch');\n# Plotted the losses vs epochs.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:16:04.921274Z","iopub.execute_input":"2022-09-01T19:16:04.921823Z","iopub.status.idle":"2022-09-01T19:16:13.951094Z","shell.execute_reply.started":"2022-09-01T19:16:04.921788Z","shell.execute_reply":"2022-09-01T19:16:13.950393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate the model\n","metadata":{}},{"cell_type":"code","source":"# TO EVALUATE THE ENTIRE TEST SET\nwith torch.no_grad():\n    y_val = model.forward(X_test)\n    loss = criterion(y_val, y_test)\nprint(f'{loss:.8f}')\n\n# The final loss was  turned out to be pretty low relative to where the model started.\n# The losses could have been minimised further but was not able able to do that because of kaggles limits.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:13:42.730263Z","iopub.execute_input":"2022-09-01T19:13:42.730556Z","iopub.status.idle":"2022-09-01T19:13:42.739371Z","shell.execute_reply.started":"2022-09-01T19:13:42.730527Z","shell.execute_reply":"2022-09-01T19:13:42.738547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\nwith torch.no_grad():\n    for i,data in enumerate(X_test):\n        y_val = model.forward(data)\n#         print(f'{i+1:2}. {str(y_val)} {y_val.argmax().item()} {y_test[i]}')\n        if y_val.argmax().item() == y_test[i]:\n            correct += 1\nprint(f'\\n{correct} out of {len(y_test)} = {100*correct/len(y_test):.2f}% correct')\n# Accuracy","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:13:51.445409Z","iopub.execute_input":"2022-09-01T19:13:51.445995Z","iopub.status.idle":"2022-09-01T19:13:59.411077Z","shell.execute_reply.started":"2022-09-01T19:13:51.445963Z","shell.execute_reply":"2022-09-01T19:13:59.410229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Got an accuracy of around 90 percent. That is pretty good!","metadata":{}},{"cell_type":"code","source":"y_val\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:17:07.033304Z","iopub.execute_input":"2022-09-01T19:17:07.033587Z","iopub.status.idle":"2022-09-01T19:17:07.041715Z","shell.execute_reply.started":"2022-09-01T19:17:07.033559Z","shell.execute_reply":"2022-09-01T19:17:07.040767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:20:48.465978Z","iopub.execute_input":"2022-09-01T19:20:48.466251Z","iopub.status.idle":"2022-09-01T19:20:48.473732Z","shell.execute_reply.started":"2022-09-01T19:20:48.466224Z","shell.execute_reply":"2022-09-01T19:20:48.473037Z"},"trusted":true},"execution_count":null,"outputs":[]}]}