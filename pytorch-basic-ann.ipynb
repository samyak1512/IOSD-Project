{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basic PyTorch Neural Network\n","metadata":{}},{"cell_type":"markdown","source":"**Note: I have done all the visualizations in another notebook( the svm one). Was not able to do the visualizations here because the Ram was occupied by the Neural Network(around 12.6 GB)**","metadata":{}},{"cell_type":"markdown","source":"**Used Pytorch for this project**","metadata":{}},{"cell_type":"markdown","source":"# Version 8","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.016117Z","iopub.execute_input":"2022-09-02T14:08:22.016689Z","iopub.status.idle":"2022-09-02T14:08:22.877394Z","shell.execute_reply.started":"2022-09-02T14:08:22.016638Z","shell.execute_reply":"2022-09-02T14:08:22.876603Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()\n# Using a GPU for faster calculations","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.879336Z","iopub.execute_input":"2022-09-02T14:08:22.879796Z","iopub.status.idle":"2022-09-02T14:08:22.968845Z","shell.execute_reply.started":"2022-09-02T14:08:22.879742Z","shell.execute_reply":"2022-09-02T14:08:22.968061Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.memory_allocated()\n# After the notebook was run numerous times, to check the memory alloted to GPU as kaggle only allows 16GB.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.970245Z","iopub.execute_input":"2022-09-02T14:08:22.970509Z","iopub.status.idle":"2022-09-02T14:08:22.990475Z","shell.execute_reply.started":"2022-09-02T14:08:22.970469Z","shell.execute_reply":"2022-09-02T14:08:22.989562Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create a model class\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features=9, h1=250, h2=250,h3=50,h4=250,h5=250, out_features=2):\n# Used a really large number of neurons for this task.\n        super().__init__()\n        self.fc1 = nn.Linear(in_features,h1)    # input layer\n        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n        self.fc3 = nn.Linear(h2, h3)            # hidden layer\n        self.fc4 = nn.Linear(h3, h4)            # hidden layer\n        self.fc5 = nn.Linear(h4, h5)            # hidden layer\n        self.out = nn.Linear(h5, out_features)  # output layer\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = self.out(x)\n        return x\n#     Used ReLu activation function. Also used Sigmoid Function earlier but was not able to get the desired results.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:22.993262Z","iopub.execute_input":"2022-09-02T14:08:22.993831Z","iopub.status.idle":"2022-09-02T14:08:23.002327Z","shell.execute_reply.started":"2022-09-02T14:08:22.993683Z","shell.execute_reply":"2022-09-02T14:08:23.001562Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Instantiated the Model class using parameter defaults:\ntorch.manual_seed(32)\nmodel = Model()\ngpumodel= model.cuda()\n# Instantiated the model on GPU","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:23.004067Z","iopub.execute_input":"2022-09-02T14:08:23.004291Z","iopub.status.idle":"2022-09-02T14:08:29.842540Z","shell.execute_reply.started":"2022-09-02T14:08:23.004269Z","shell.execute_reply":"2022-09-02T14:08:29.841654Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Load the iris dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nasa-nearest-earth-objects/neo.csv')\ndf = df.drop('name',axis=1)\ndf = df.drop('sentry_object',axis=1)\ndf = df.drop('orbiting_body',axis=1)\ndf = df.drop('id',axis=1)\n# Dropping not so important classes. As sentry object is false and orbiting object is Earth.\n\n#  Feature Engineering to add more accuracy.\ndf[\"hazardous\"] = df[\"hazardous\"].astype(int)\ndf['Mass'] = df.loc[:, 'absolute_magnitude'] ** (1/4)\ndf['avg_radius'] = (df.loc[:, 'est_diameter_min'] + df.loc[:,'est_diameter_max'])/2\ndf['Volume'] = (df.loc[:, 'avg_radius'] ** 3)*1.33\ndf['energy'] = (df.loc[:, 'Mass'] * df.loc[:, 'relative_velocity']**2)*0.5\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:29.844158Z","iopub.execute_input":"2022-09-02T14:08:29.844528Z","iopub.status.idle":"2022-09-02T14:08:30.176542Z","shell.execute_reply.started":"2022-09-02T14:08:29.844497Z","shell.execute_reply":"2022-09-02T14:08:30.175747Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   est_diameter_min  est_diameter_max  relative_velocity  miss_distance  \\\n0          1.198271          2.679415       13569.249224   5.483974e+07   \n1          0.265800          0.594347       73588.726663   6.143813e+07   \n2          0.722030          1.614507      114258.692129   4.979872e+07   \n3          0.096506          0.215794       24764.303138   2.543497e+07   \n4          0.255009          0.570217       42737.733765   4.627557e+07   \n\n   absolute_magnitude  hazardous      Mass  avg_radius    Volume        energy  \n0               16.73          0  2.022432    1.938843  9.693475  1.861897e+08  \n1               20.00          1  2.114743    0.430073  0.105798  5.725983e+09  \n2               17.83          0  2.054886    1.168268  2.120701  1.341332e+10  \n3               22.20          0  2.170642    0.156150  0.005064  6.655956e+08  \n4               20.09          1  2.117118    0.412613  0.093429  1.933472e+09  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>est_diameter_min</th>\n      <th>est_diameter_max</th>\n      <th>relative_velocity</th>\n      <th>miss_distance</th>\n      <th>absolute_magnitude</th>\n      <th>hazardous</th>\n      <th>Mass</th>\n      <th>avg_radius</th>\n      <th>Volume</th>\n      <th>energy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.198271</td>\n      <td>2.679415</td>\n      <td>13569.249224</td>\n      <td>5.483974e+07</td>\n      <td>16.73</td>\n      <td>0</td>\n      <td>2.022432</td>\n      <td>1.938843</td>\n      <td>9.693475</td>\n      <td>1.861897e+08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.265800</td>\n      <td>0.594347</td>\n      <td>73588.726663</td>\n      <td>6.143813e+07</td>\n      <td>20.00</td>\n      <td>1</td>\n      <td>2.114743</td>\n      <td>0.430073</td>\n      <td>0.105798</td>\n      <td>5.725983e+09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.722030</td>\n      <td>1.614507</td>\n      <td>114258.692129</td>\n      <td>4.979872e+07</td>\n      <td>17.83</td>\n      <td>0</td>\n      <td>2.054886</td>\n      <td>1.168268</td>\n      <td>2.120701</td>\n      <td>1.341332e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.096506</td>\n      <td>0.215794</td>\n      <td>24764.303138</td>\n      <td>2.543497e+07</td>\n      <td>22.20</td>\n      <td>0</td>\n      <td>2.170642</td>\n      <td>0.156150</td>\n      <td>0.005064</td>\n      <td>6.655956e+08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.255009</td>\n      <td>0.570217</td>\n      <td>42737.733765</td>\n      <td>4.627557e+07</td>\n      <td>20.09</td>\n      <td>1</td>\n      <td>2.117118</td>\n      <td>0.412613</td>\n      <td>0.093429</td>\n      <td>1.933472e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Perform Train/Test/Split","metadata":{}},{"cell_type":"code","source":"# Using the scikit learn library.\nX = df.drop('hazardous',axis=1).values\n# from sklearn.preprocessing import StandardScaler\n# sc = StandardScaler()\n# X = sc.fit_transform(X)\ny = df['hazardous'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=33)\n\nX_train = torch.FloatTensor(X_train).cuda()\nX_test = torch.FloatTensor(X_test).cuda()\n# y_train = F.one_hot(torch.LongTensor(y_train))\n# y_test = F.one_hot(torch.LongTensor(y_test))\n# Not needed with cross entropy loss, was earlier implementing BCELoss but was facing error while converting tensors from long to float.\ny_train = torch.LongTensor(y_train).cuda()\ny_test = torch.LongTensor(y_test).cuda()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.177943Z","iopub.execute_input":"2022-09-02T14:08:30.178227Z","iopub.status.idle":"2022-09-02T14:08:30.210533Z","shell.execute_reply.started":"2022-09-02T14:08:30.178179Z","shell.execute_reply":"2022-09-02T14:08:30.209845Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.213557Z","iopub.execute_input":"2022-09-02T14:08:30.213760Z","iopub.status.idle":"2022-09-02T14:08:30.237270Z","shell.execute_reply.started":"2022-09-02T14:08:30.213737Z","shell.execute_reply":"2022-09-02T14:08:30.236610Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([0, 1, 0,  ..., 0, 0, 0], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ndata = df.drop('hazardous',axis=1).values\nlabels = df['hazardous'].values\nclass_weights =[]\n\nDataset = TensorDataset(torch.FloatTensor(data).cuda(),torch.LongTensor(labels).cuda())","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.238370Z","iopub.execute_input":"2022-09-02T14:08:30.239265Z","iopub.status.idle":"2022-09-02T14:08:30.250547Z","shell.execute_reply.started":"2022-09-02T14:08:30.239227Z","shell.execute_reply":"2022-09-02T14:08:30.249856Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"Data_loader = DataLoader(Dataset, batch_size=105, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T14:08:30.254040Z","iopub.execute_input":"2022-09-02T14:08:30.254281Z","iopub.status.idle":"2022-09-02T14:08:30.259755Z","shell.execute_reply.started":"2022-09-02T14:08:30.254249Z","shell.execute_reply":"2022-09-02T14:08:30.258833Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# !pip install torchsampler","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:26:40.420974Z","iopub.execute_input":"2022-09-01T14:26:40.421489Z","iopub.status.idle":"2022-09-01T14:26:40.429634Z","shell.execute_reply.started":"2022-09-01T14:26:40.421457Z","shell.execute_reply":"2022-09-01T14:26:40.428707Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# from torchsampler import ImbalancedDatasetSampler\n\n# train_loader = torch.utils.data.DataLoader(\n#     iris,\n#     sampler=ImbalancedDatasetSampler(iris),\n#     batch_size=1000,\n#     **kwargs\n# )\n\n# Was trying to use Imbalance Data Sampler from github but faced some error, so manually entered class weights.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:26:40.431168Z","iopub.execute_input":"2022-09-01T14:26:40.432124Z","iopub.status.idle":"2022-09-01T14:26:40.439332Z","shell.execute_reply.started":"2022-09-01T14:26:40.432086Z","shell.execute_reply":"2022-09-01T14:26:40.438579Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## Define loss equations and optimizations\n","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(4)\nmodel = Model()\ngpumodel= model.cuda()\nweight = torch.FloatTensor([0.9,0.09]).cuda()\n# Assigned weights to classes as the data provided was highly imbalanced.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:15.293132Z","iopub.execute_input":"2022-09-01T14:28:15.293437Z","iopub.status.idle":"2022-09-01T14:28:15.303707Z","shell.execute_reply.started":"2022-09-01T14:28:15.293394Z","shell.execute_reply":"2022-09-01T14:28:15.302864Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"weight","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:16.346442Z","iopub.execute_input":"2022-09-01T14:28:16.347129Z","iopub.status.idle":"2022-09-01T14:28:16.357199Z","shell.execute_reply.started":"2022-09-01T14:28:16.347096Z","shell.execute_reply":"2022-09-01T14:28:16.356345Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"tensor([0.9000, 0.0900], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight = weight).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000000001)\n# Since the dataset was highly imbalanced was forced to use a extremely low learning rate or else the model was predicting all the values as either 1 or 0.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:18.513425Z","iopub.execute_input":"2022-09-01T14:28:18.513990Z","iopub.status.idle":"2022-09-01T14:28:18.519879Z","shell.execute_reply.started":"2022-09-01T14:28:18.513957Z","shell.execute_reply":"2022-09-01T14:28:18.518513Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"epochs = 3000000\nlosses = []\n# Used too many epochs to further lower down the losses.\n# The training went for around 9 hours using Kaggle Tesla GPUs.\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model.forward(X_train)\n    loss = criterion(y_pred, y_train)\n    losses.append(loss)\n    \n    # a neat trick to save screen space:\n    if i%1000 == 1:\n        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T14:28:19.322983Z","iopub.execute_input":"2022-09-01T14:28:19.323261Z","iopub.status.idle":"2022-09-01T19:12:49.348394Z","shell.execute_reply.started":"2022-09-01T14:28:19.323231Z","shell.execute_reply":"2022-09-01T19:12:49.346485Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"epoch:  1  loss: 13113465.00000000\nepoch: 1001  loss: 13073929.00000000\nepoch: 2001  loss: 13034348.00000000\nepoch: 3001  loss: 12994747.00000000\nepoch: 4001  loss: 12955099.00000000\nepoch: 5001  loss: 12915415.00000000\nepoch: 6001  loss: 12875700.00000000\nepoch: 7001  loss: 12835948.00000000\nepoch: 8001  loss: 12796162.00000000\nepoch: 9001  loss: 12756348.00000000\nepoch: 10001  loss: 12716508.00000000\nepoch: 11001  loss: 12676645.00000000\nepoch: 12001  loss: 12636759.00000000\nepoch: 13001  loss: 12596855.00000000\nepoch: 14001  loss: 12556925.00000000\nepoch: 15001  loss: 12516978.00000000\nepoch: 16001  loss: 12477010.00000000\nepoch: 17001  loss: 12437038.00000000\nepoch: 18001  loss: 12397044.00000000\nepoch: 19001  loss: 12357042.00000000\nepoch: 20001  loss: 12317022.00000000\nepoch: 21001  loss: 12276988.00000000\nepoch: 22001  loss: 12236946.00000000\nepoch: 23001  loss: 12196895.00000000\nepoch: 24001  loss: 12156824.00000000\nepoch: 25001  loss: 12116734.00000000\nepoch: 26001  loss: 12076648.00000000\nepoch: 27001  loss: 12036540.00000000\nepoch: 28001  loss: 11996422.00000000\nepoch: 29001  loss: 11956302.00000000\nepoch: 30001  loss: 11916170.00000000\nepoch: 31001  loss: 11876030.00000000\nepoch: 32001  loss: 11835876.00000000\nepoch: 33001  loss: 11795724.00000000\nepoch: 34001  loss: 11755553.00000000\nepoch: 35001  loss: 11715383.00000000\nepoch: 36001  loss: 11675199.00000000\nepoch: 37001  loss: 11635022.00000000\nepoch: 38001  loss: 11594830.00000000\nepoch: 39001  loss: 11554650.00000000\nepoch: 40001  loss: 11514460.00000000\nepoch: 41001  loss: 11474276.00000000\nepoch: 42001  loss: 11434108.00000000\nepoch: 43001  loss: 11393939.00000000\nepoch: 44001  loss: 11353787.00000000\nepoch: 45001  loss: 11313635.00000000\nepoch: 46001  loss: 11273511.00000000\nepoch: 47001  loss: 11233392.00000000\nepoch: 48001  loss: 11193291.00000000\nepoch: 49001  loss: 11153201.00000000\nepoch: 50001  loss: 11113122.00000000\nepoch: 51001  loss: 11073053.00000000\nepoch: 52001  loss: 11032990.00000000\nepoch: 53001  loss: 10992936.00000000\nepoch: 54001  loss: 10952889.00000000\nepoch: 55001  loss: 10912854.00000000\nepoch: 56001  loss: 10872823.00000000\nepoch: 57001  loss: 10832804.00000000\nepoch: 58001  loss: 10792793.00000000\nepoch: 59001  loss: 10752787.00000000\nepoch: 60001  loss: 10712796.00000000\nepoch: 61001  loss: 10672819.00000000\nepoch: 62001  loss: 10632848.00000000\nepoch: 63001  loss: 10592898.00000000\nepoch: 64001  loss: 10552955.00000000\nepoch: 65001  loss: 10513016.00000000\nepoch: 66001  loss: 10473090.00000000\nepoch: 67001  loss: 10433152.00000000\nepoch: 68001  loss: 10393209.00000000\nepoch: 69001  loss: 10353274.00000000\nepoch: 70001  loss: 10313326.00000000\nepoch: 71001  loss: 10273380.00000000\nepoch: 72001  loss: 10233434.00000000\nepoch: 73001  loss: 10193482.00000000\nepoch: 74001  loss: 10153506.00000000\nepoch: 75001  loss: 10113426.00000000\nepoch: 76001  loss: 10073059.00000000\nepoch: 77001  loss: 10032251.00000000\nepoch: 78001  loss: 9991137.00000000\nepoch: 79001  loss: 9949953.00000000\nepoch: 80001  loss: 9908763.00000000\nepoch: 81001  loss: 9867568.00000000\nepoch: 82001  loss: 9826377.00000000\nepoch: 83001  loss: 9785198.00000000\nepoch: 84001  loss: 9744023.00000000\nepoch: 85001  loss: 9702858.00000000\nepoch: 86001  loss: 9661701.00000000\nepoch: 87001  loss: 9620555.00000000\nepoch: 88001  loss: 9579415.00000000\nepoch: 89001  loss: 9538283.00000000\nepoch: 90001  loss: 9497163.00000000\nepoch: 91001  loss: 9456059.00000000\nepoch: 92001  loss: 9414967.00000000\nepoch: 93001  loss: 9373880.00000000\nepoch: 94001  loss: 9332798.00000000\nepoch: 95001  loss: 9291726.00000000\nepoch: 96001  loss: 9250667.00000000\nepoch: 97001  loss: 9209614.00000000\nepoch: 98001  loss: 9168572.00000000\nepoch: 99001  loss: 9127536.00000000\nepoch: 100001  loss: 9086508.00000000\nepoch: 101001  loss: 9045492.00000000\nepoch: 102001  loss: 9004486.00000000\nepoch: 103001  loss: 8963481.00000000\nepoch: 104001  loss: 8922483.00000000\nepoch: 105001  loss: 8881494.00000000\nepoch: 106001  loss: 8840511.00000000\nepoch: 107001  loss: 8799536.00000000\nepoch: 108001  loss: 8758565.00000000\nepoch: 109001  loss: 8717600.00000000\nepoch: 110001  loss: 8676639.00000000\nepoch: 111001  loss: 8635687.00000000\nepoch: 112001  loss: 8594738.00000000\nepoch: 113001  loss: 8553791.00000000\nepoch: 114001  loss: 8512844.00000000\nepoch: 115001  loss: 8471900.00000000\nepoch: 116001  loss: 8430968.00000000\nepoch: 117001  loss: 8390039.00000000\nepoch: 118001  loss: 8349121.00000000\nepoch: 119001  loss: 8308212.00000000\nepoch: 120001  loss: 8267307.00000000\nepoch: 121001  loss: 8226406.00000000\nepoch: 122001  loss: 8185455.00000000\nepoch: 123001  loss: 8144803.50000000\nepoch: 124001  loss: 8103533.50000000\nepoch: 125001  loss: 8061879.00000000\nepoch: 126001  loss: 8020100.50000000\nepoch: 127001  loss: 7978275.50000000\nepoch: 128001  loss: 7936420.00000000\nepoch: 129001  loss: 7894448.50000000\nepoch: 130001  loss: 7852380.00000000\nepoch: 131001  loss: 7810195.50000000\nepoch: 132001  loss: 7767863.00000000\nepoch: 133001  loss: 7725379.50000000\nepoch: 134001  loss: 7682747.00000000\nepoch: 135001  loss: 7639975.00000000\nepoch: 136001  loss: 7597091.50000000\nepoch: 137001  loss: 7554117.50000000\nepoch: 138001  loss: 7511079.00000000\nepoch: 139001  loss: 7467991.50000000\nepoch: 140001  loss: 7424862.50000000\nepoch: 141001  loss: 7381704.50000000\nepoch: 142001  loss: 7338525.00000000\nepoch: 143001  loss: 7295345.00000000\nepoch: 144001  loss: 7252101.50000000\nepoch: 145001  loss: 7208618.00000000\nepoch: 146001  loss: 7164873.50000000\nepoch: 147001  loss: 7120742.50000000\nepoch: 148001  loss: 7076148.00000000\nepoch: 149001  loss: 7031196.50000000\nepoch: 150001  loss: 6985958.00000000\nepoch: 151001  loss: 6940475.00000000\nepoch: 152001  loss: 6894861.00000000\nepoch: 153001  loss: 6849120.50000000\nepoch: 154001  loss: 6803276.50000000\nepoch: 155001  loss: 6757377.50000000\nepoch: 156001  loss: 6711432.00000000\nepoch: 157001  loss: 6665457.50000000\nepoch: 158001  loss: 6619466.00000000\nepoch: 159001  loss: 6573465.50000000\nepoch: 160001  loss: 6527452.00000000\nepoch: 161001  loss: 6481429.00000000\nepoch: 162001  loss: 6435405.50000000\nepoch: 163001  loss: 6389370.00000000\nepoch: 164001  loss: 6343330.50000000\nepoch: 165001  loss: 6297291.00000000\nepoch: 166001  loss: 6251266.50000000\nepoch: 167001  loss: 6205249.00000000\nepoch: 168001  loss: 6159246.00000000\nepoch: 169001  loss: 6113232.50000000\nepoch: 170001  loss: 6067228.00000000\nepoch: 171001  loss: 6021223.50000000\nepoch: 172001  loss: 5975219.50000000\nepoch: 173001  loss: 5929222.00000000\nepoch: 174001  loss: 5883224.50000000\nepoch: 175001  loss: 5837235.50000000\nepoch: 176001  loss: 5791247.50000000\nepoch: 177001  loss: 5745263.00000000\nepoch: 178001  loss: 5699285.00000000\nepoch: 179001  loss: 5653310.00000000\nepoch: 180001  loss: 5607341.50000000\nepoch: 181001  loss: 5561377.50000000\nepoch: 182001  loss: 5515418.50000000\nepoch: 183001  loss: 5469469.50000000\nepoch: 184001  loss: 5423528.00000000\nepoch: 185001  loss: 5377592.50000000\nepoch: 186001  loss: 5331657.50000000\nepoch: 187001  loss: 5285717.00000000\nepoch: 188001  loss: 5239778.00000000\nepoch: 189001  loss: 5193829.50000000\nepoch: 190001  loss: 5147879.00000000\nepoch: 191001  loss: 5101917.50000000\nepoch: 192001  loss: 5055946.00000000\nepoch: 193001  loss: 5009975.00000000\nepoch: 194001  loss: 4964002.00000000\nepoch: 195001  loss: 4918034.00000000\nepoch: 196001  loss: 4872067.50000000\nepoch: 197001  loss: 4826100.00000000\nepoch: 198001  loss: 4780130.50000000\nepoch: 199001  loss: 4734162.50000000\nepoch: 200001  loss: 4688193.50000000\nepoch: 201001  loss: 4642226.00000000\nepoch: 202001  loss: 4596268.50000000\nepoch: 203001  loss: 4550305.00000000\nepoch: 204001  loss: 4504339.50000000\nepoch: 205001  loss: 4458363.00000000\nepoch: 206001  loss: 4412382.00000000\nepoch: 207001  loss: 4366397.50000000\nepoch: 208001  loss: 4320422.00000000\nepoch: 209001  loss: 4274449.00000000\nepoch: 210001  loss: 4228479.00000000\nepoch: 211001  loss: 4182512.00000000\nepoch: 212001  loss: 4136544.00000000\nepoch: 213001  loss: 4090577.25000000\nepoch: 214001  loss: 4044607.75000000\nepoch: 215001  loss: 3998636.75000000\nepoch: 216001  loss: 3952661.25000000\nepoch: 217001  loss: 3906679.25000000\nepoch: 218001  loss: 3860689.00000000\nepoch: 219001  loss: 3814696.50000000\nepoch: 220001  loss: 3768695.75000000\nepoch: 221001  loss: 3722681.00000000\nepoch: 222001  loss: 3676646.75000000\nepoch: 223001  loss: 3630602.00000000\nepoch: 224001  loss: 3584539.75000000\nepoch: 225001  loss: 3538457.75000000\nepoch: 226001  loss: 3492364.25000000\nepoch: 227001  loss: 3446255.50000000\nepoch: 228001  loss: 3400116.00000000\nepoch: 229001  loss: 3353941.00000000\nepoch: 230001  loss: 3307725.00000000\nepoch: 231001  loss: 3261453.75000000\nepoch: 232001  loss: 3215130.75000000\nepoch: 233001  loss: 3168742.00000000\nepoch: 234001  loss: 3122276.25000000\nepoch: 235001  loss: 3075720.50000000\nepoch: 236001  loss: 3029067.75000000\nepoch: 237001  loss: 2982334.50000000\nepoch: 238001  loss: 2935495.00000000\nepoch: 239001  loss: 2888566.25000000\nepoch: 240001  loss: 2841546.00000000\nepoch: 241001  loss: 2794485.00000000\nepoch: 242001  loss: 2747419.25000000\nepoch: 243001  loss: 2700343.50000000\nepoch: 244001  loss: 2653249.75000000\nepoch: 245001  loss: 2606138.75000000\nepoch: 246001  loss: 2559018.00000000\nepoch: 247001  loss: 2511890.00000000\nepoch: 248001  loss: 2464754.00000000\nepoch: 249001  loss: 2417605.50000000\nepoch: 250001  loss: 2370445.25000000\nepoch: 251001  loss: 2323274.25000000\nepoch: 252001  loss: 2276090.75000000\nepoch: 253001  loss: 2228902.00000000\nepoch: 254001  loss: 2181712.25000000\nepoch: 255001  loss: 2134523.75000000\nepoch: 256001  loss: 2087334.50000000\nepoch: 257001  loss: 2040142.50000000\nepoch: 258001  loss: 1992951.37500000\nepoch: 259001  loss: 1945762.12500000\nepoch: 260001  loss: 1898441.00000000\nepoch: 261001  loss: 1851039.00000000\nepoch: 262001  loss: 1803554.12500000\nepoch: 263001  loss: 1756044.25000000\nepoch: 264001  loss: 1708532.75000000\nepoch: 265001  loss: 1661017.12500000\nepoch: 266001  loss: 1613506.12500000\nepoch: 267001  loss: 1565997.50000000\nepoch: 268001  loss: 1518490.75000000\nepoch: 269001  loss: 1470985.12500000\nepoch: 270001  loss: 1423481.87500000\nepoch: 271001  loss: 1375981.50000000\nepoch: 272001  loss: 1328484.25000000\nepoch: 273001  loss: 1280990.62500000\nepoch: 274001  loss: 1233503.37500000\nepoch: 275001  loss: 1186020.87500000\nepoch: 276001  loss: 1138544.87500000\nepoch: 277001  loss: 1091077.62500000\nepoch: 278001  loss: 1043618.12500000\nepoch: 279001  loss: 996161.87500000\nepoch: 280001  loss: 948705.06250000\nepoch: 281001  loss: 901255.56250000\nepoch: 282001  loss: 853832.87500000\nepoch: 283001  loss: 806434.25000000\nepoch: 284001  loss: 759052.56250000\nepoch: 285001  loss: 711702.81250000\nepoch: 286001  loss: 664413.75000000\nepoch: 287001  loss: 617200.93750000\nepoch: 288001  loss: 570014.00000000\nepoch: 289001  loss: 522868.65625000\nepoch: 290001  loss: 475830.40625000\nepoch: 291001  loss: 428932.87500000\nepoch: 292001  loss: 382121.46875000\nepoch: 293001  loss: 335418.21875000\nepoch: 294001  loss: 288893.37500000\nepoch: 295001  loss: 242476.85937500\nepoch: 296001  loss: 196119.09375000\nepoch: 297001  loss: 162832.23437500\nepoch: 298001  loss: 148506.39062500\nepoch: 299001  loss: 136502.46875000\nepoch: 300001  loss: 125817.45312500\nepoch: 301001  loss: 116357.44531250\nepoch: 302001  loss: 108039.21093750\nepoch: 303001  loss: 100773.27343750\nepoch: 304001  loss: 94416.15625000\nepoch: 305001  loss: 88837.31250000\nepoch: 306001  loss: 83945.24218750\nepoch: 307001  loss: 79668.80468750\nepoch: 308001  loss: 75930.08593750\nepoch: 309001  loss: 72656.37500000\nepoch: 310001  loss: 69777.94531250\nepoch: 311001  loss: 67233.45312500\nepoch: 312001  loss: 64968.78906250\nepoch: 313001  loss: 62950.33593750\nepoch: 314001  loss: 61128.25000000\nepoch: 315001  loss: 59477.29687500\nepoch: 316001  loss: 57987.65625000\nepoch: 317001  loss: 56634.56640625\nepoch: 318001  loss: 55392.66406250\nepoch: 319001  loss: 54195.76171875\nepoch: 320001  loss: 53040.64062500\nepoch: 321001  loss: 51951.87500000\nepoch: 322001  loss: 50902.49609375\nepoch: 323001  loss: 49909.72656250\nepoch: 324001  loss: 48960.40625000\nepoch: 325001  loss: 48072.84375000\nepoch: 326001  loss: 47231.74218750\nepoch: 327001  loss: 46413.32031250\nepoch: 328001  loss: 45605.36718750\nepoch: 329001  loss: 44802.56640625\nepoch: 330001  loss: 44092.15234375\nepoch: 331001  loss: 43411.87500000\nepoch: 332001  loss: 42778.38281250\nepoch: 333001  loss: 42191.05468750\nepoch: 334001  loss: 41610.13671875\nepoch: 335001  loss: 41051.27734375\nepoch: 336001  loss: 40524.29687500\nepoch: 337001  loss: 40035.20312500\nepoch: 338001  loss: 39544.78515625\nepoch: 339001  loss: 39080.36718750\nepoch: 340001  loss: 38656.07812500\nepoch: 341001  loss: 38281.57421875\nepoch: 342001  loss: 37928.44140625\nepoch: 343001  loss: 37581.84765625\nepoch: 344001  loss: 37252.76562500\nepoch: 345001  loss: 36945.88281250\nepoch: 346001  loss: 36635.51953125\nepoch: 347001  loss: 36342.62109375\nepoch: 348001  loss: 36066.49218750\nepoch: 349001  loss: 35802.14453125\nepoch: 350001  loss: 35540.32031250\nepoch: 351001  loss: 35285.63671875\nepoch: 352001  loss: 35047.53125000\nepoch: 353001  loss: 34810.15625000\nepoch: 354001  loss: 34579.02734375\nepoch: 355001  loss: 34356.52734375\nepoch: 356001  loss: 34147.91796875\nepoch: 357001  loss: 33936.53515625\nepoch: 358001  loss: 33721.82031250\nepoch: 359001  loss: 33509.53906250\nepoch: 360001  loss: 33298.20703125\nepoch: 361001  loss: 33059.94140625\nepoch: 362001  loss: 32835.88671875\nepoch: 363001  loss: 32609.39453125\nepoch: 364001  loss: 32385.15429688\nepoch: 365001  loss: 32166.43945312\nepoch: 366001  loss: 31947.00976562\nepoch: 367001  loss: 31729.05078125\nepoch: 368001  loss: 31512.32226562\nepoch: 369001  loss: 31296.46679688\nepoch: 370001  loss: 31082.60937500\nepoch: 371001  loss: 30870.83007812\nepoch: 372001  loss: 30660.96484375\nepoch: 373001  loss: 30452.48632812\nepoch: 374001  loss: 30245.17968750\nepoch: 375001  loss: 30038.94140625\nepoch: 376001  loss: 29833.78320312\nepoch: 377001  loss: 29628.47656250\nepoch: 378001  loss: 29423.19531250\nepoch: 379001  loss: 29218.82617188\nepoch: 380001  loss: 29013.06835938\nepoch: 381001  loss: 28807.64648438\nepoch: 382001  loss: 28602.37304688\nepoch: 383001  loss: 28396.48046875\nepoch: 384001  loss: 28189.82031250\nepoch: 385001  loss: 27983.34570312\nepoch: 386001  loss: 27778.03515625\nepoch: 387001  loss: 27574.01757812\nepoch: 388001  loss: 27368.77539062\nepoch: 389001  loss: 27161.69921875\nepoch: 405001  loss: 23727.25390625\nepoch: 406001  loss: 23530.94335938\nepoch: 407001  loss: 23336.01953125\nepoch: 408001  loss: 23140.29296875\nepoch: 409001  loss: 22942.60546875\nepoch: 410001  loss: 22743.86132812\nepoch: 411001  loss: 22542.70312500\nepoch: 412001  loss: 22339.01171875\nepoch: 413001  loss: 22134.36718750\nepoch: 414001  loss: 21930.04492188\nepoch: 415001  loss: 21725.26953125\nepoch: 416001  loss: 21521.00781250\nepoch: 417001  loss: 21317.11914062\nepoch: 418001  loss: 21113.77539062\nepoch: 419001  loss: 20911.05468750\nepoch: 420001  loss: 20708.87890625\nepoch: 421001  loss: 20507.17773438\nepoch: 422001  loss: 20305.99609375\nepoch: 423001  loss: 20105.38281250\nepoch: 424001  loss: 19905.41406250\nepoch: 425001  loss: 19705.80273438\nepoch: 426001  loss: 19506.71093750\nepoch: 427001  loss: 19308.15625000\nepoch: 428001  loss: 19110.18554688\nepoch: 429001  loss: 18912.74609375\nepoch: 430001  loss: 18715.77929688\nepoch: 431001  loss: 18519.44921875\nepoch: 432001  loss: 18323.66210938\nepoch: 433001  loss: 18128.33398438\nepoch: 434001  loss: 17933.54101562\nepoch: 435001  loss: 17739.91015625\nepoch: 436001  loss: 17548.67382812\nepoch: 437001  loss: 17359.45507812\nepoch: 438001  loss: 17172.52734375\nepoch: 439001  loss: 16989.85156250\nepoch: 440001  loss: 16807.16796875\nepoch: 441001  loss: 16625.41992188\nepoch: 442001  loss: 16444.70703125\nepoch: 443001  loss: 16265.82714844\nepoch: 444001  loss: 16087.86816406\nepoch: 445001  loss: 15911.81054688\nepoch: 446001  loss: 15737.04785156\nepoch: 447001  loss: 15563.31152344\nepoch: 448001  loss: 15391.01171875\nepoch: 449001  loss: 15218.26660156\nepoch: 450001  loss: 15047.86523438\nepoch: 451001  loss: 14879.96093750\nepoch: 452001  loss: 14714.12109375\nepoch: 453001  loss: 14550.21972656\nepoch: 454001  loss: 14387.79785156\nepoch: 455001  loss: 14226.52441406\nepoch: 456001  loss: 14066.60546875\nepoch: 457001  loss: 13908.12597656\nepoch: 458001  loss: 13751.00488281\nepoch: 459001  loss: 13595.06933594\nepoch: 460001  loss: 13440.13671875\nepoch: 461001  loss: 13286.35644531\nepoch: 462001  loss: 13133.70703125\nepoch: 463001  loss: 12982.03222656\nepoch: 464001  loss: 12831.45800781\nepoch: 465001  loss: 12681.93164062\nepoch: 466001  loss: 12533.36718750\nepoch: 467001  loss: 12385.78027344\nepoch: 468001  loss: 12239.06640625\nepoch: 469001  loss: 12093.29589844\nepoch: 470001  loss: 11948.31933594\nepoch: 471001  loss: 11804.16992188\nepoch: 472001  loss: 11660.91601562\nepoch: 473001  loss: 11518.62402344\nepoch: 474001  loss: 11377.19140625\nepoch: 475001  loss: 11236.53808594\nepoch: 476001  loss: 11096.71484375\nepoch: 477001  loss: 10957.64941406\nepoch: 478001  loss: 10819.37695312\nepoch: 479001  loss: 10682.01562500\nepoch: 480001  loss: 10545.31542969\nepoch: 481001  loss: 10409.17773438\nepoch: 482001  loss: 10273.66113281\nepoch: 483001  loss: 10138.67578125\nepoch: 484001  loss: 10004.40722656\nepoch: 485001  loss: 9870.73339844\nepoch: 486001  loss: 9737.59082031\nepoch: 487001  loss: 9604.99023438\nepoch: 488001  loss: 9472.81152344\nepoch: 489001  loss: 9341.20507812\nepoch: 490001  loss: 9210.10839844\nepoch: 491001  loss: 9079.33496094\nepoch: 492001  loss: 8948.98535156\nepoch: 493001  loss: 8819.47167969\nepoch: 494001  loss: 8691.51660156\nepoch: 495001  loss: 8562.28808594\nepoch: 496001  loss: 8432.22558594\nepoch: 497001  loss: 8302.11425781\nepoch: 498001  loss: 8172.25048828\nepoch: 499001  loss: 8042.82324219\nepoch: 500001  loss: 7913.76513672\nepoch: 501001  loss: 7785.04882812\nepoch: 502001  loss: 7656.61230469\nepoch: 503001  loss: 7528.49267578\nepoch: 504001  loss: 7400.75195312\nepoch: 505001  loss: 7273.39257812\nepoch: 506001  loss: 7146.33349609\nepoch: 507001  loss: 7019.56689453\nepoch: 508001  loss: 6893.12792969\nepoch: 509001  loss: 6767.02490234\nepoch: 510001  loss: 6641.23437500\nepoch: 511001  loss: 6515.67089844\nepoch: 512001  loss: 6390.38574219\nepoch: 513001  loss: 6265.31542969\nepoch: 514001  loss: 6140.46240234\nepoch: 515001  loss: 6015.85595703\nepoch: 516001  loss: 5891.49414062\nepoch: 517001  loss: 5767.36914062\nepoch: 518001  loss: 5643.48828125\nepoch: 519001  loss: 5519.88720703\nepoch: 520001  loss: 5396.58544922\nepoch: 521001  loss: 5273.63525391\nepoch: 522001  loss: 5151.49023438\nepoch: 523001  loss: 5029.95800781\nepoch: 524001  loss: 4907.75732422\nepoch: 525001  loss: 4784.35205078\nepoch: 526001  loss: 4659.70263672\nepoch: 527001  loss: 4534.15722656\nepoch: 528001  loss: 4406.97509766\nepoch: 529001  loss: 4280.08642578\nepoch: 530001  loss: 4151.96142578\nepoch: 531001  loss: 4023.45507812\nepoch: 532001  loss: 3894.96411133\nepoch: 533001  loss: 3766.66845703\nepoch: 534001  loss: 3638.65527344\nepoch: 535001  loss: 3511.57543945\nepoch: 536001  loss: 3385.40527344\nepoch: 537001  loss: 3260.13647461\nepoch: 538001  loss: 3135.88159180\nepoch: 539001  loss: 3012.88134766\nepoch: 540001  loss: 2891.39038086\nepoch: 541001  loss: 2771.53002930\nepoch: 542001  loss: 2652.61376953\nepoch: 543001  loss: 2534.76025391\nepoch: 544001  loss: 2425.90136719\nepoch: 545001  loss: 2357.87280273\nepoch: 546001  loss: 2308.70263672\nepoch: 547001  loss: 2265.55810547\nepoch: 548001  loss: 2222.70043945\nepoch: 549001  loss: 2180.70263672\nepoch: 550001  loss: 2139.11645508\nepoch: 551001  loss: 2097.29785156\nepoch: 552001  loss: 2054.04345703\nepoch: 553001  loss: 2007.70385742\nepoch: 554001  loss: 1967.72265625\nepoch: 555001  loss: 1929.92968750\nepoch: 556001  loss: 1892.56958008\nepoch: 557001  loss: 1857.72912598\nepoch: 558001  loss: 1827.12231445\nepoch: 559001  loss: 1798.54260254\nepoch: 560001  loss: 1772.92211914\nepoch: 561001  loss: 1761.14306641\nepoch: 562001  loss: 1747.74426270\nepoch: 563001  loss: 1731.23205566\nepoch: 564001  loss: 1711.30261230\nepoch: 565001  loss: 1688.88037109\nepoch: 566001  loss: 1664.11926270\nepoch: 567001  loss: 1638.02490234\nepoch: 568001  loss: 1611.06616211\nepoch: 569001  loss: 1583.49096680\nepoch: 570001  loss: 1555.62866211\nepoch: 571001  loss: 1527.99377441\nepoch: 572001  loss: 1500.86486816\nepoch: 573001  loss: 1473.60009766\nepoch: 574001  loss: 1448.33435059\nepoch: 575001  loss: 1424.07055664\nepoch: 576001  loss: 1398.31042480\nepoch: 577001  loss: 1372.84997559\nepoch: 578001  loss: 1348.64379883\nepoch: 579001  loss: 1324.31445312\nepoch: 580001  loss: 1299.96484375\nepoch: 581001  loss: 1275.80480957\nepoch: 582001  loss: 1252.38000488\nepoch: 583001  loss: 1228.59655762\nepoch: 584001  loss: 1205.37060547\nepoch: 585001  loss: 1182.27172852\nepoch: 586001  loss: 1160.77990723\nepoch: 587001  loss: 1140.25573730\nepoch: 588001  loss: 1119.09423828\nepoch: 589001  loss: 1098.86437988\nepoch: 590001  loss: 1078.75769043\nepoch: 591001  loss: 1060.39099121\nepoch: 592001  loss: 1041.49426270\nepoch: 593001  loss: 1022.69726562\nepoch: 594001  loss: 1005.07641602\nepoch: 595001  loss: 987.19598389\nepoch: 596001  loss: 970.48040771\nepoch: 597001  loss: 953.54382324\nepoch: 598001  loss: 939.62298584\nepoch: 599001  loss: 925.56219482\nepoch: 600001  loss: 911.55291748\nepoch: 601001  loss: 897.68426514\nepoch: 602001  loss: 884.78924561\nepoch: 603001  loss: 878.29205322\nepoch: 604001  loss: 870.38610840\nepoch: 605001  loss: 860.65069580\nepoch: 606001  loss: 850.39776611\nepoch: 607001  loss: 840.74511719\nepoch: 608001  loss: 830.61285400\nepoch: 609001  loss: 819.68896484\nepoch: 610001  loss: 808.18359375\nepoch: 611001  loss: 796.48242188\nepoch: 612001  loss: 784.44952393\nepoch: 613001  loss: 772.17535400\nepoch: 614001  loss: 759.89733887\nepoch: 615001  loss: 747.55841064\nepoch: 616001  loss: 735.01593018\nepoch: 617001  loss: 722.49096680\nepoch: 618001  loss: 709.55786133\nepoch: 619001  loss: 696.63470459\nepoch: 620001  loss: 683.77941895\nepoch: 621001  loss: 671.01629639\nepoch: 622001  loss: 658.64916992\nepoch: 623001  loss: 645.96075439\nepoch: 624001  loss: 633.92016602\nepoch: 625001  loss: 621.96789551\nepoch: 626001  loss: 610.04101562\nepoch: 627001  loss: 598.32031250\nepoch: 628001  loss: 586.88604736\nepoch: 629001  loss: 575.38385010\nepoch: 630001  loss: 563.90484619\nepoch: 631001  loss: 552.81854248\nepoch: 632001  loss: 541.71923828\nepoch: 633001  loss: 530.75585938\nepoch: 634001  loss: 520.06585693\nepoch: 635001  loss: 509.68719482\nepoch: 636001  loss: 499.69393921\nepoch: 637001  loss: 490.23822021\nepoch: 638001  loss: 480.67700195\nepoch: 639001  loss: 472.01776123\nepoch: 640001  loss: 463.02465820\nepoch: 641001  loss: 453.57318115\nepoch: 642001  loss: 444.32717896\nepoch: 643001  loss: 435.63552856\nepoch: 644001  loss: 427.14944458\nepoch: 645001  loss: 419.09725952\nepoch: 646001  loss: 413.69793701\nepoch: 647001  loss: 410.41802979\nepoch: 648001  loss: 406.66604614\nepoch: 649001  loss: 402.07534790\nepoch: 650001  loss: 396.56359863\nepoch: 651001  loss: 390.50650024\nepoch: 652001  loss: 384.17163086\nepoch: 653001  loss: 377.81481934\nepoch: 654001  loss: 371.34204102\nepoch: 655001  loss: 365.30017090\nepoch: 656001  loss: 358.91519165\nepoch: 657001  loss: 352.59829712\nepoch: 658001  loss: 346.41946411\nepoch: 659001  loss: 340.40750122\nepoch: 660001  loss: 334.65707397\nepoch: 661001  loss: 329.31875610\nepoch: 662001  loss: 324.24179077\nepoch: 663001  loss: 319.13729858\nepoch: 664001  loss: 313.87368774\nepoch: 665001  loss: 308.48928833\nepoch: 666001  loss: 303.03115845\nepoch: 667001  loss: 297.44924927\nepoch: 668001  loss: 291.99594116\nepoch: 669001  loss: 286.53659058\nepoch: 670001  loss: 281.62158203\nepoch: 671001  loss: 276.56860352\nepoch: 672001  loss: 271.25341797\nepoch: 673001  loss: 265.88473511\nepoch: 674001  loss: 260.73898315\nepoch: 675001  loss: 255.61450195\nepoch: 676001  loss: 250.36718750\nepoch: 677001  loss: 245.30068970\nepoch: 678001  loss: 240.25967407\nepoch: 679001  loss: 235.27212524\nepoch: 680001  loss: 230.26040649\nepoch: 681001  loss: 225.44863892\nepoch: 682001  loss: 220.75460815\nepoch: 683001  loss: 216.08470154\nepoch: 684001  loss: 211.49848938\nepoch: 685001  loss: 206.87730408\nepoch: 686001  loss: 202.37364197\nepoch: 687001  loss: 198.98141479\nepoch: 688001  loss: 195.77548218\nepoch: 689001  loss: 191.82626343\nepoch: 690001  loss: 187.68392944\nepoch: 691001  loss: 183.52565002\nepoch: 692001  loss: 179.44706726\nepoch: 693001  loss: 175.35659790\nepoch: 694001  loss: 171.44493103\nepoch: 695001  loss: 167.70574951\nepoch: 696001  loss: 164.02426147\nepoch: 697001  loss: 160.37654114\nepoch: 698001  loss: 156.85261536\nepoch: 699001  loss: 153.40545654\nepoch: 700001  loss: 149.73832703\nepoch: 701001  loss: 146.33805847\nepoch: 702001  loss: 142.85418701\nepoch: 703001  loss: 139.26687622\nepoch: 704001  loss: 135.89645386\nepoch: 705001  loss: 132.62831116\nepoch: 706001  loss: 129.29083252\nepoch: 707001  loss: 126.09790039\nepoch: 708001  loss: 123.02138519\nepoch: 709001  loss: 119.98136139\nepoch: 710001  loss: 116.96562958\nepoch: 711001  loss: 114.13007355\nepoch: 712001  loss: 111.23738861\nepoch: 713001  loss: 108.40725708\nepoch: 714001  loss: 105.62739563\nepoch: 715001  loss: 102.91073608\nepoch: 716001  loss: 100.39976501\nepoch: 717001  loss: 97.85446167\nepoch: 718001  loss: 95.32704163\nepoch: 719001  loss: 92.73715973\nepoch: 720001  loss: 90.23417664\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/2630905730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Plot the loss function","metadata":{}},{"cell_type":"code","source":"epochs = 720465","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:16:03.890422Z","iopub.execute_input":"2022-09-01T19:16:03.891145Z","iopub.status.idle":"2022-09-01T19:16:03.895959Z","shell.execute_reply.started":"2022-09-01T19:16:03.891112Z","shell.execute_reply":"2022-09-01T19:16:03.894044Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(epochs), losses)\nplt.ylabel('Loss')\nplt.xlabel('epoch');\n# Plotted the losses vs epochs.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:16:04.921274Z","iopub.execute_input":"2022-09-01T19:16:04.921823Z","iopub.status.idle":"2022-09-01T19:16:13.951094Z","shell.execute_reply.started":"2022-09-01T19:16:04.921788Z","shell.execute_reply":"2022-09-01T19:16:13.950393Z"},"trusted":true},"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Validate the model\n","metadata":{}},{"cell_type":"code","source":"# TO EVALUATE THE ENTIRE TEST SET\nwith torch.no_grad():\n    y_val = model.forward(X_test)\n    loss = criterion(y_val, y_test)\nprint(f'{loss:.8f}')\n\n# The final loss was  turned out to be pretty low relative to where the model started.\n# The losses could have been minimised further but was not able able to do that because of kaggles limits.","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:13:42.730263Z","iopub.execute_input":"2022-09-01T19:13:42.730556Z","iopub.status.idle":"2022-09-01T19:13:42.739371Z","shell.execute_reply.started":"2022-09-01T19:13:42.730527Z","shell.execute_reply":"2022-09-01T19:13:42.738547Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"91.94483948\n","output_type":"stream"}]},{"cell_type":"code","source":"correct = 0\nwith torch.no_grad():\n    for i,data in enumerate(X_test):\n        y_val = model.forward(data)\n#         print(f'{i+1:2}. {str(y_val)} {y_val.argmax().item()} {y_test[i]}')\n        if y_val.argmax().item() == y_test[i]:\n            correct += 1\nprint(f'\\n{correct} out of {len(y_test)} = {100*correct/len(y_test):.2f}% correct')\n# Accuracy","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:13:51.445409Z","iopub.execute_input":"2022-09-01T19:13:51.445995Z","iopub.status.idle":"2022-09-01T19:13:59.411077Z","shell.execute_reply.started":"2022-09-01T19:13:51.445963Z","shell.execute_reply":"2022-09-01T19:13:59.410229Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"\n16206 out of 18168 = 89.20% correct\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Got an accuracy of around 90 percent. That is pretty good!","metadata":{}},{"cell_type":"code","source":"y_val\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:17:07.033304Z","iopub.execute_input":"2022-09-01T19:17:07.033587Z","iopub.status.idle":"2022-09-01T19:17:07.041715Z","shell.execute_reply.started":"2022-09-01T19:17:07.033559Z","shell.execute_reply":"2022-09-01T19:17:07.040767Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"tensor([-6528206., -6531831.], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-09-01T19:20:48.465978Z","iopub.execute_input":"2022-09-01T19:20:48.466251Z","iopub.status.idle":"2022-09-01T19:20:48.473732Z","shell.execute_reply.started":"2022-09-01T19:20:48.466224Z","shell.execute_reply":"2022-09-01T19:20:48.473037Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')"},"metadata":{}}]}]}